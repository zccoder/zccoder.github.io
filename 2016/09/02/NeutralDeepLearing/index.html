<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="卷积神经网络的理解（CNN）
人工智能，机器学习以及深度学习；等基本概念基本概念神经网络的发展

　　作为一名对深度学习完全陌生的小白，按照个人理解将有些抽象深奥的理论转变成容易接受的是我写一系列博客的主要目的。
人工智能，机器学习及深度学习等基本概念　　Artificial Intelligence，也就是人工智能，就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/2016/09/02/NeutralDeepLearing/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="卷积神经网络的理解（CNN）
人工智能，机器学习以及深度学习；等基本概念基本概念神经网络的发展

　　作为一名对深度学习完全陌生的小白，按照个人理解将有些抽象深奥的理论转变成容易接受的是我写一系列博客的主要目的。
人工智能，机器学习及深度学习等基本概念　　Artificial Intelligence，也就是人工智能，就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足">
<meta property="og:updated_time" content="2016-09-02T13:49:36.774Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
<meta name="twitter:description" content="卷积神经网络的理解（CNN）
人工智能，机器学习以及深度学习；等基本概念基本概念神经网络的发展

　　作为一名对深度学习完全陌生的小白，按照个人理解将有些抽象深奥的理论转变成容易接受的是我写一系列博客的主要目的。
人工智能，机器学习及深度学习等基本概念　　Artificial Intelligence，也就是人工智能，就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-NeutralDeepLearing" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/09/02/NeutralDeepLearing/" class="article-date">
  <time datetime="2016-09-02T13:49:36.878Z" itemprop="datePublished">2016-09-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="卷积神经网络的理解（CNN）"><a href="#卷积神经网络的理解（CNN）" class="headerlink" title="卷积神经网络的理解（CNN）"></a>卷积神经网络的理解（CNN）</h1><blockquote>
<p>人工智能，机器学习以及深度学习；<br>等基本概念<br>基本概念<br>神经网络的发展</p>
</blockquote>
<p>　　作为一名对深度学习完全陌生的小白，按照个人理解将有些抽象深奥的理论转变成容易接受的是我写一系列博客的主要目的。</p>
<h2 id="人工智能，机器学习及深度学习等基本概念"><a href="#人工智能，机器学习及深度学习等基本概念" class="headerlink" title="人工智能，机器学习及深度学习等基本概念"></a>人工智能，机器学习及深度学习等基本概念</h2><p>　　Artificial Intelligence，也就是人工智能，就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足的进步，但是到目前为止，还没有一台电脑能产生“自我”的意识。<br>　　但是自 2006 年以来，机器学习领域，取得了突破性的进展。图灵试验，至少不是那么可望而不可及了。至于技术手段，不仅仅依赖于云计算对大数据的并行处理能力，而且依赖于算法。这个算法就是，Deep Learning。借助于 Deep Learning 算法，人类终于找到了如何处理“抽象概念”这个亘古难题的方法。<br>　　机器学习（MachineLearning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题。例如图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等等。<br>　　传统的识别方法：从开始的通过传感器（例如CMOS）来获得数据。然后经过预处理、特征提取、特征选择，再到推理、预测或者识别。而中间的三部分，概括起来就是特征表达。良好的特征表达，对最终算法的准确性起了非常关键的作用，而且系统主要的计算和测试工作都耗在这一大部分。但，这块实际中一般都是人工完成的。Deep Learning就是用来干这个事情的，看它的一个别名UnsupervisedFeature Learning，就可以顾名思义了，Unsupervised的意思就是不要人参与特征的选取过程。</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="特征表示的粒度"><a href="#特征表示的粒度" class="headerlink" title="特征表示的粒度"></a>特征表示的粒度</h3><p>　　特征是机器学习系统的原材料，对最终模型的影响是毋庸置疑的。就一个图片来说，像素级的特征根本没有价值。而如果特征是一个具有结构性（或者说有含义）的时候，比如是否具有车把手（handle），是否具有车轮（wheel），就很容易把摩托车和非摩托车区分，学习算法才能发挥作用。</p>
<h3 id="初级（浅层）特征表示"><a href="#初级（浅层）特征表示" class="headerlink" title="初级（浅层）特征表示"></a>初级（浅层）特征表示</h3><p>   复杂图形，往往由一些基本结构组成。大牛们还发现，不仅图像存在这个规律，声音也存在。他们从未标注的声音中发现了20种基本的声音结构，其余的声音可以由这20种基本结构合成。</p>
<h3 id="结构性特征表示"><a href="#结构性特征表示" class="headerlink" title="结构性特征表示"></a>结构性特征表示</h3><p>   V2看V1是像素级，这个是层次递进的，高层表达由底层表达的组合而成。专业点说就是基basis。V1取提出的basis是边缘，然后V2层是V1层这些basis的组合，这时候V2区得到的又是高一层的basis。即上一层的basis组合的结果，上上层又是上一层的组合basis。<br>   直观上说，就是找到makesense的小patch再将其进行combine，就得到了上一层的feature，递归地向上learningfeature。在不同object上做training是，所得的edge basis 是非常相似的，但object parts和models 就会completely different了。</p>
<h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p>　　浅层学习是机器学习的第一次浪潮。0世纪80年代末期，用于人工神经网络的反向传播算法（也叫Back　Propagation算法或者BP算法）的发明，给机器学习带来了希望，掀起了基于统计模型的机器学习热潮。这个热潮一直持续到今天。20世纪90年代，各种各样的浅层机器学习模型相继被提出，例如支撑向量机（SVM，Support Vector Machines）、Boosting、最大熵方法（如LR，LogisticRegression）等。这些模型的结构基本上可以看成带有一层隐层节点（如SVM、Boosting），或没有隐层节点（如LR）。这些模型无论是在理论分析还是应用中都获得了巨大的成功。相比之下，由于理论分析的难度大，训练方法又需要很多经验和技巧，这个时期浅层人工神经网络反而相对沉寂。<br>　　深度学习是机器学习的第二次浪潮。这篇文章有两个主要观点：1）多隐层的人工神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；2）深度神经网络在训练上的难度，可以通过“逐层初始化”（layer-wise pre-training）来有效克服，在这篇文章中，逐层初始化是通过无监督学习实现的。深度学习可通过学习一种深层非线性网络结构，实现复杂函数逼近，表征输入数据分布式表示，并展现了强大的从少数样本集中学习数据集本质特征的能力。（多层的好处是可以用较少的参数表示复杂的函数）<br>　　区别于传统的浅层学习，深度学习的不同在于：1）强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；2）明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。<br>　　深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。深度学习是无监督学习的一种。<br>　　Deep learning本身算是machine learning的一个分支，简单可以理解为neural network的发展。大约二三十年前，neural 　network曾经是ML领域特别火热的一个方向，但是后来确慢慢淡出了，原因包括以下几个方面：<br>1）比较容易过拟合，参数比较难tune，而且需要不少trick；<br>2）训练速度比较慢，在层次比较少（小于等于3）的情况下效果并不比其它方法更优；</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>   在图像处理中，往往把图像表示为像素的向量，比如一个1000×1000的图像，可以表示为一个1000000的向量。如果隐含层数目与输入层一样，即也是1000000时，那么输入层到隐含层的参数数据为1000000×1000000=10^12，这样就太多了，基本没法训练。所以图像处理要想练成神经网络大法，必先减少参数加快速度。</p>
<p>###局部感知<br>   假如每个神经元只和10×10个像素值相连，那么权值数据为1000000×100个参数，减少为原来的千分之一。而那10×10个像素值对应的10×10个参数，其实就相当于卷积操作。</p>
<p>###深度学习训练过程<br>　　如果对所有层同时训练，时间复杂度会太高；如果每次训练一层，偏差就会逐层传递。这会面临跟上面监督学习中相反的问题，会严重欠拟合（因为深度网络的神经元和参数太多了）。<br>　 　2006年，hinton提出了在非监督数据上建立多层神经网络的一个有效方法，简单的说，分为两步，一是每次训练一层网络，二是调优，使原始表示x向上生成的高级表示r和该高级表示r向下生成的x’尽可能一致。方法是：<br>1）首先逐层构建单层神经元，这样每次都是训练一个单层网络。<br>2）当所有层训练完后，Hinton使用wake-sleep算法进行调优。<br>　　将除最顶层的其它层间的权重变为双向的，这样最顶层仍然是一个单层神经网络，而其它层则变为了图模型。向上的权重用于“认知”，向下的权重用于“生成”。然后使用Wake-Sleep算法调整所有的权重。让认知和生成达成一致，也就是保证生成的最顶层表示能够尽可能正确的复原底层的结点。比如顶层的一个结点表示人脸，那么所有人脸的图像应该激活这个结点，并且这个结果向下生成的图像应该能够表现为一个大概的人脸图像。Wake-Sleep算法分为醒（wake）和睡（sleep）两个部分。<br>1）wake阶段：认知过程，通过外界的特征和向上的权重（认知权重）产生每一层的抽象表示（结点状态），并且使用梯度下降修改层间的下行权重（生成权重）。也就是“如果现实跟我想象的不一样，改变我的权重使得我想象的东西就是这样的”<br>2）sleep阶段：生成过程，通过顶层表示（醒时学得的概念）和向下权重，生成底层的状态，同时修改层间向上的权重。也就是“如果梦中的景象不是我脑中的相应概念，改变我的认知权重使得这种景象在我看来就是这个概念”。</p>
<p>deep learning训练过程具体如下：<br>1）使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）：<br>　　采用无标定数据（有标定数据也可）分层训练各层参数，这一步可以看作是一个无监督训练过程，是和传统神经网络区别最大的部分（这个过程可以看作是feature learning过程）：<br>　　具体的，先用无标定数据训练第一层，训练时先学习第一层的参数（这一层可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层），由于模型capacity的限制以及稀疏性约束，使得得到的模型能够学习到数据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到第n-1层后，将n-1层的输出作为第n层的输入，训练第n层，由此分别得到各层的参数；<br>2）自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）：<br>　　基于第一步得到的各层参数进一步fine-tune整个多层模型的参数，这一步是一个有监督训练过程；第一步类似神经网络的随机初始化初值过程，由于DL的第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果；所以deeplearning效果好很大程度上归功于第一步的feature learning过程。</p>
<h2 id="Deep-Learning的常用模型或者方法"><a href="#Deep-Learning的常用模型或者方法" class="headerlink" title="Deep Learning的常用模型或者方法"></a>Deep Learning的常用模型或者方法</h2><p>   Deep Learning最简单的一种方法是利用人工神经网络的特点，人工神经网络（ANN）本身就是具有层次结构的系统，如果给定一个神经网络，我们假设其输出与输入是相同的，然后训练调整其参数，得到每一层中的权重。自然地，我们就得到了输入I的几种不同表示（每一层代表一种表示），这些表示就是特征。自动编码器就是一种尽可能复现输入信号的神经网络。为了实现这种复现，自动编码器就必须捕捉可以代表输入数据的最重要的因素，就像PCA那样，找到可以代表原信息的主要成分。<br>   具体过程简单的说明如下：<br>1）给定无标签数据，用非监督学习学习特征：<br>2）通过编码器产生特征，然后训练下一层。这样逐层训练：<br>3）有监督微调：<br>  AutoEncoder存在一些变体，这里简要介绍下两个：<br>  Sparse AutoEncoder稀疏自动编码器,DenoisingAutoEncoders降噪自动编码器：</p>
<p>##　卷积神经网络<br>　　神经网络的训练方法也同Logistic类似，不过由于其多层性，还需要利用链式求导法则对隐含层的节点进行求导，即梯度下降+链式求导法则，专业名称为反向传播。<br>（１）局部感知<br>　　卷积神经网络有两种神器可以降低参数数目，第一种神器叫做局部感知野。一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。网络部分连通的思想，也是受启发于生物学里面的视觉系统结构。<br>（２）参数共享<br>   　　　我们可以这100个参数（也就是卷积操作）看成是提取特征的方式，该方式与位置无关。这其中隐含的原理则是：图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。<br>   更直观一些，当从一个大尺寸图像中随机选取一小块，比如说 8×8 作为样本，并且从这个小块样本中学习到了一些特征，这时我们可以把从这个 8×8 样本中学习到的特征作为探测器，应用到这个图像的任意地方中去。特别是，我们可以用从8×8样本中所学习到的特征跟原本的大尺寸图像作卷积，从而对这个大尺寸图像上的任一位置获得一个不同特征的激活值。</p>
<p>###多卷积核</p>
<p>###Down-pooling<br>   在通过卷积获得了特征(features)之后，下一步我们希望利用这些特征去做分类。例如：对于一个96X96像素的图像，假设我们已经学习得到了400个定义在8X8输入上的特征，每一个特征和图像卷积都会得到一个 (96 −8 + 1) × (96 − 8 + 1)=7921维的卷积特征，由于有400个特征，所以每个样例 (example) 都会得到一个892×4003,168,4（３） 多卷积核<br>　　上面所述只有100个参数时，表明只有1个100*100维的卷积特征向量。学习一个拥有超过 3 百万特征输入的分类器十分不便，并且容易出现过拟合 (over-fitting)。<br>   为了描述大的图像，一个很自然的想法就是对不同位置的特征进行聚合统计，例如，人们可以计算图像一个区域上的某个特定特征的平均值(或最大值)。这些概要统计特征不仅具有低得多的维度(相比使用所有提取得到的特征)，同时还会改善结果(不容易过拟合)。这种聚合的操作就叫做池化(pooling)，有时也称为平均池化或者最大池化(取决于计算池化的方法)。</p>
<h3 id="多层卷积"><a href="#多层卷积" class="headerlink" title="多层卷积"></a>多层卷积</h3><p>####ImageNet-2010网络结构</p>
<p>####DeepID网络结构</p>
<p>核，显然，特征提取是不充分的，我们可以添加多个卷积核，比如32个卷积核，可以学习32种特征。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/09/02/NeutralDeepLearing/" data-id="cislthyzf0002a7jxamajqhyj" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2016/05/30/hexoUsing/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">hexoUsing</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/09/02/NeutralDeepLearing/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/05/30/hexoUsing/">hexoUsing</a>
          </li>
        
          <li>
            <a href="/2016/05/13/hello/">hello</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>